{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Hypercube - Preparación de ficheros de datos para la NN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Procesos para preparar los datos extraidos de Matlab con el script CreateMaskForMaterials.m y generar los ficheros de entrenamiento y test para la red neuronal.\n",
    "\n",
    "El directorio donde se están ubicando todos los .csv que se extraen de los hypercubos capturados con la cámara es C:\\\\_DATA\\\\JOVISA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from FileGenerator import BatchGenerator\n",
    "from FileGenerator import ValidationGenerator\n",
    "from FileGenerator import SpectrumsCounter\n",
    "from FileGenerator import FilesNumLines\n",
    "from FileGenerator import FileNumLines\n",
    "from neural_network_Estimator_v3 import RegressionHyperModel_1Layer\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow import keras, optimizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import HyperModel\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Definición de funciones de utilidad en FileGenerator.py\n",
    "\n",
    "### BatchGenerator\n",
    "En FileGenerator.py tenemos el BatchGenerator que nos proporciona los batches de espectros desde los ficheros. Lo configuramos para que cada vez que se usa next sobre el generador, se devuelva un único espectro, tanto X como Y.\n",
    "\n",
    "    Nota: En la versión original usada para las termicas teníamos uno para train y otro para test, pero como se le pasa el FileList que componen cada uno de los conjuntos el funcionamiento es el mismo. Aquí sólo hemos dejado el BatchGenerator como genérico.\n",
    "\n",
    "Tenemos también varias funcione de utilidad\n",
    "\n",
    "### Función NumLines\n",
    "Creamos una función para calcular el número de líneas de un fichero.\n",
    "La usaremos para definir las líneas por Chunk que utilizarmos para partir los ficheros en los mismos trozos\n",
    "\n",
    "### SpectrumsCounter\n",
    "Dado un conjutno de ficheros nos dice el número de líneas (espectros) en conjunto que tiene. (hay que ver si con NumLines va más rápido)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Establecimiento de Path y lista de ficheros a procesar\n",
    "Definimios el path de donde cogemos los ficheros .csv generados por Matlab, donde los espectros ya están preprocesados.\n",
    "Este path es diferente en funciòn del ordenador donde estemos ejecutando, según los comentarios de la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Path de los CSV preprocesados\n",
    "#Descomentar el que proceda.\n",
    "\n",
    "#PATHS EN HCUBESERVER\n",
    "#path = r'C:\\_DATA\\JOVISA\\CSV'\n",
    "#pathSplit = r'C:\\_DATA\\JOVISA\\CSV\\SPLIT' #\n",
    "#trainPath = r'C:\\_DATA\\JOVISA\\CSV\\SPLIT\\TRAIN'\n",
    "#testPath = r'C:\\_DATA\\JOVISA\\CSV\\SPLIT\\TEST'\n",
    "\n",
    "#PATHS CINTA VERDE\n",
    "path = r'.'\n",
    "pathSplit = r'.\\SPLIT'\n",
    "trainPath = r'.\\TRAIN'\n",
    "testPath = r'.\\TEST'\n",
    "#PATHS DATOS CINTA NEGRA\n",
    "#path = r'D:\\_DATA\\JOVISA_CSV_FOLDER\\CSV\\raw'\n",
    "#pathSplit = r'D:\\_DATA\\JOVISA_CSV_FOLDER\\CSV\\SPLIT'\n",
    "#trainPath = r'D:\\_DATA\\JOVISA_CSV_FOLDER\\CSV\\SPLIT\\TRAIN'\n",
    "#testPath = r'D:\\_DATA\\JOVISA_CSV_FOLDER\\CSV\\SPLIT\\TEST'\n",
    "#PATHS EN PORTATIL\n",
    "#path = r'C:\\_DATA\\JOVISA\\CSV'\n",
    "#pathSplit = r'C:\\_DATA\\JOVISA\\CSV\\SPLIT' #\n",
    "#trainPath = r'C:\\_DATA\\JOVISA\\CSV\\SPLIT\\TRAIN'\n",
    "#testPath = r'C:\\_DATA\\JOVISA\\CSV\\SPLIT\\TEST'\n",
    "\n",
    "#all_files = glob.glob(os.path.join(path , \"tap-pet_roi_from_botellas2a80pct_0_preprocesed.csv\"))\n",
    "all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "#print(all_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Generación de ficheros con muestras de materiales\n",
    "\n",
    "Los ficheros que matlab genera con los espectros tiene un tipo de material por fichero.\n",
    "Hay que generar una secuencia aleatoria de espectros, de forma que la NN tenga muestras aleatoriamente generadas para el conjunto de entrenamiento y de test.\n",
    "\n",
    "Como primer generamos un conjunto de NFiles ficheros cuyo contenido se obtiene de los ficheros .csv preprocesados de todos los materiales, añadiendo proporcionalmente (al numero de filas del fichero) el número de filas de cada uno de los materiales.\n",
    "\n",
    "Para saber el número de lineas de un fichero usamos la función NumLines que hemos definido antes.\n",
    "Vamos a generar nFiles=100 ficheros en el directorio .\\SPLIT.\n",
    "Cada fichero tendrá un trozo de cada uno de los ficheros .csv, para que haya muestras de cada material en todos los ficheros generados.\n",
    "Para saber de un fichero cuantas lineas hay que coger para generar 100 trozos (chunks) usamos: math.ceil(nLines/nFiles).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TROCEA LOS FICHEROS PREPROCESADOS EN nFiles TROZOS, GRABANDO los nuevos ficheros con parte de cada uno de los originales\n",
    "nFiles=100\n",
    "for filename in all_files:\n",
    "    cCounter=0\n",
    "    nLines= FileNumLines(filename)\n",
    "    linesChunks=math.ceil(nLines/nFiles)\n",
    "    print(\"Procesando \"+filename)\n",
    "    with pd.read_csv(filename, sep=\";\", index_col=None, header=None, chunksize=linesChunks) as reader:\n",
    "        for chunk in reader:\n",
    "            oFile=pathSplit+\"\\\\f\"+str(cCounter)+\".csv\"\n",
    "            #print(oFile)\n",
    "            #print(chunk.size)\n",
    "            chunk.to_csv(oFile, mode='a',sep=\";\", header=False, index=False)\n",
    "            cCounter+=1\n",
    "        #print(cCounter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Reordenación aleatoria de los espectros en los ficheros\n",
    "Una vez que tenemos los ficheros en el directorio SPLIT, estos tienen un bloque de lineas consecutivas de espectros para cada tipo.\n",
    "Lo que hay que hacer es modificar el orden de las líneas para que sea aleatorio.\n",
    "Se modifica el orden aleatorio sobreescribiendo el fichero en el directorio de SPLIT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#de los ficheros trozeados anteriormente, los carga, reordena aleatoriamente y vuelve a grabarlos (sobreescribe)\n",
    "splitFiles= glob.glob(os.path.join(pathSplit , \"*.csv\"))\n",
    "#print(splitFiles)\n",
    "for filename in splitFiles:\n",
    "    df= pd.read_csv(filename, sep=\";\", index_col=None, header=None)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df.to_csv(filename, index=False,sep=\";\", header=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Generación de Training y Test Files\n",
    "\n",
    "1. Crear directorios de train y test\n",
    "2. Ordenar aleatoriamente los nFiles\n",
    "3. Determinar cuantos van a train y a test y copiarlos a su dir\n",
    "4. Definir el BatchGenerator y ValidationGenerator. Modificar FileGenerator.py para ello.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. y 2. Creados y ordenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splitFiles= glob.glob(os.path.join(pathSplit , \"*.csv\"))\n",
    "random.shuffle(splitFiles)\n",
    "print(splitFiles)\n",
    "#Definimos los porcentajes de test y train\n",
    "pctTrain=.8\n",
    "\n",
    "trainFiles=splitFiles[0:int(len(splitFiles)*pctTrain)]\n",
    "testFiles=splitFiles[int(len(splitFiles)*pctTrain):len(splitFiles)]\n",
    "\n",
    "#print(len(trainFiles))\n",
    "#print(len(testFiles))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3. cuantos y moverlos a los directorios TRAIN y TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for filename in trainFiles:\n",
    "    basename = os.path.basename(filename)\n",
    "    os.rename(filename, trainPath + \"\\\\\" + basename)\n",
    "\n",
    "for filename in testFiles:\n",
    "    basename = os.path.basename(filename)\n",
    "    os.rename(filename, testPath + \"\\\\\" + basename)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Definición de variables y Generador\n",
    "\n",
    "- Cargamos las variables **trainFiles** y **testFiles** con el conjunto respectivo de ficheros para entrenamiento y ficheros para validación o test.\n",
    "- Definimios un BatchGenerador temporal para capturar el número de Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#definimos las listas de ficheros\n",
    "trainFiles= glob.glob(os.path.join(trainPath , \"f*.csv\"))\n",
    "testFiles= glob.glob(os.path.join(testPath , \"f*.csv\"))\n",
    "#print(trainFiles[0])\n",
    "#Definimos el generador temporal con un único fichero para poder capturar luego los parámetros o features de la red.\n",
    "#filelist=[trainFiles[0]]\n",
    "filelist=trainFiles\n",
    "#print(filelist)\n",
    "batchsize=1000\n",
    "traingenerator = BatchGenerator(filelist, batchsize, ';', validation=False)\n",
    "print(traingenerator)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Obtenemos el numero de parametros de entrada y el numero de tipos de materiales diferentes a partir del BatchGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Obtenemos el número de parametros usando el generaodr definido con un único fichero.\n",
    "#Al usar el generador nos da los dos valores y ya no los recuperamos, por lo que habrá que redefinir el generador más adelante.\n",
    "x,y=next(traingenerator)\n",
    "#print(x,y)\n",
    "n_features=x.shape[1]\n",
    "print(f\"Número de Features: {n_features}\")\n",
    "n_clases=np.unique(y).shape[0]\n",
    "print(n_clases)\n",
    "#REINICIAMOS EL TRAINGENERATOR POR HABER PERDIDO LA PRIMERA EJECUCION\n",
    "traingenerator = BatchGenerator(filelist, batchsize, ';', validation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to process: 80\n",
      "Separator ;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fran Quiles\\Documents\\Tfg_IA\\hipercubo\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to process: 20\n",
      "Separator ;\n",
      "(204,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fran Quiles\\Documents\\Tfg_IA\\hipercubo\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#definimos las listas de ficheros\n",
    "trainFiles= glob.glob(os.path.join(trainPath , \"f*.csv\"))\n",
    "testFiles= glob.glob(os.path.join(testPath , \"f*.csv\"))\n",
    "batchsize=1000\n",
    "traingenerator = BatchGenerator(trainFiles, batchsize, ';', validation=False)\n",
    "testgenerator = BatchGenerator(testFiles, batchsize, ';', validation=False)\n",
    "\n",
    "\n",
    "X_train,Y_train=next(traingenerator)\n",
    "X_test,Y_test=next(testgenerator)\n",
    "\n",
    "data_augmentation = False\n",
    "only_size = None; # Puede ser [None, 4, 8, 16, 32, 64]\n",
    "\n",
    "normalization = False\n",
    "standarization = True\n",
    "\n",
    "add_MDV = False\n",
    "add_statistics = True\n",
    "add_sizes = True\n",
    "model_type=1\n",
    "# if standarization or normalization:\n",
    "#     if standarization:\n",
    "#         scaler = StandardScaler()\n",
    "#     else:\n",
    "#         scaler = MinMaxScaler()\n",
    "#     X_train = scaler.fit_transform(X_train)\n",
    "#     X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "input_shapex = (X_train.shape[1],)\n",
    "print(input_shapex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204,)\n",
      "<neural_network_Estimator_v3.RegressionHyperModel_1Layer object at 0x000001D5146159D0>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     hypermodel_1 \u001b[39m=\u001b[39m RegressionHyperModel_1Layer(input_shape\u001b[39m=\u001b[39minput_shapex)\n\u001b[0;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(hypermodel_1)\n\u001b[1;32m----> 5\u001b[0m     tuner_hb1 \u001b[39m=\u001b[39m kt\u001b[39m.\u001b[39;49mHyperband(\n\u001b[0;32m      6\u001b[0m                     hypermodel_1,\n\u001b[0;32m      7\u001b[0m                     objective\u001b[39m=\u001b[39;49mkt\u001b[39m.\u001b[39;49mObjective(\u001b[39m\"\u001b[39;49m\u001b[39mval_recall\u001b[39;49m\u001b[39m\"\u001b[39;49m, direction\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m      8\u001b[0m                     max_epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m                     factor\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[0;32m     10\u001b[0m                     directory\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmy_dir_model1\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     11\u001b[0m                     project_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mEstimator_v3\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     12\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOption not available\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Fran Quiles\\Documents\\Tfg_IA\\hipercubo\\Lib\\site-packages\\keras_tuner\\tuners\\hyperband.py:418\u001b[0m, in \u001b[0;36mHyperband.__init__\u001b[1;34m(self, hypermodel, objective, max_epochs, factor, hyperband_iterations, seed, hyperparameters, tune_new_entries, allow_new_entries, max_retries_per_trial, max_consecutive_failed_trials, **kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    392\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    393\u001b[0m     hypermodel\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    405\u001b[0m ):\n\u001b[0;32m    406\u001b[0m     oracle \u001b[39m=\u001b[39m HyperbandOracle(\n\u001b[0;32m    407\u001b[0m         objective,\n\u001b[0;32m    408\u001b[0m         max_epochs\u001b[39m=\u001b[39mmax_epochs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m         max_consecutive_failed_trials\u001b[39m=\u001b[39mmax_consecutive_failed_trials,\n\u001b[0;32m    417\u001b[0m     )\n\u001b[1;32m--> 418\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(oracle\u001b[39m=\u001b[39;49moracle, hypermodel\u001b[39m=\u001b[39;49mhypermodel, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Fran Quiles\\Documents\\Tfg_IA\\hipercubo\\Lib\\site-packages\\keras_tuner\\engine\\tuner.py:113\u001b[0m, in \u001b[0;36mTuner.__init__\u001b[1;34m(self, oracle, hypermodel, max_model_size, optimizer, loss, metrics, distribution_strategy, directory, project_name, logger, tuner_id, overwrite, executions_per_trial, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m hypermodel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39mrun_trial \u001b[39mis\u001b[39;00m Tuner\u001b[39m.\u001b[39mrun_trial:\n\u001b[0;32m    106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    107\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mReceived `hypermodel=None`. We only allow not specifying \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`hypermodel` if the user defines the search space in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`Tuner.run_trial()` by subclassing a `Tuner` class without \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39musing a `HyperModel` instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m     )\n\u001b[1;32m--> 113\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    114\u001b[0m     oracle\u001b[39m=\u001b[39;49moracle,\n\u001b[0;32m    115\u001b[0m     hypermodel\u001b[39m=\u001b[39;49mhypermodel,\n\u001b[0;32m    116\u001b[0m     directory\u001b[39m=\u001b[39;49mdirectory,\n\u001b[0;32m    117\u001b[0m     project_name\u001b[39m=\u001b[39;49mproject_name,\n\u001b[0;32m    118\u001b[0m     logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[0;32m    119\u001b[0m     overwrite\u001b[39m=\u001b[39;49moverwrite,\n\u001b[0;32m    120\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    121\u001b[0m )\n\u001b[0;32m    123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_model_size \u001b[39m=\u001b[39m max_model_size\n\u001b[0;32m    124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m optimizer\n",
      "File \u001b[1;32mc:\\Users\\Fran Quiles\\Documents\\Tfg_IA\\hipercubo\\Lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:133\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[1;34m(self, oracle, hypermodel, directory, project_name, overwrite, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreload()\n\u001b[0;32m    131\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m     \u001b[39m# Only populate initial space if not reloading.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_populate_initial_space()\n\u001b[0;32m    135\u001b[0m \u001b[39m# Run in distributed mode.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39mif\u001b[39;00m dist_utils\u001b[39m.\u001b[39mis_chief_oracle():\n\u001b[0;32m    137\u001b[0m     \u001b[39m# Blocks forever.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[39m# Avoid import at the top, to avoid inconsistent protobuf versions.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Fran Quiles\\Documents\\Tfg_IA\\hipercubo\\Lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:204\u001b[0m, in \u001b[0;36mBaseTuner._populate_initial_space\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhypermodel\u001b[39m.\u001b[39mdeclare_hyperparameters(hp)\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mupdate_space(hp)\n\u001b[1;32m--> 204\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_activate_all_conditions()\n",
      "File \u001b[1;32mc:\\Users\\Fran Quiles\\Documents\\Tfg_IA\\hipercubo\\Lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:161\u001b[0m, in \u001b[0;36mBaseTuner._activate_all_conditions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m hp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mget_space()\n\u001b[0;32m    160\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhypermodel\u001b[39m.\u001b[39;49mbuild(hp)\n\u001b[0;32m    162\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mupdate_space(hp)\n\u001b[0;32m    164\u001b[0m     \u001b[39m# Update the recorded scopes.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Fran Quiles\\Documents\\Tfg_IA\\neural_network_Estimator_v3.py:37\u001b[0m, in \u001b[0;36mRegressionHyperModel_1Layer.build\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m     26\u001b[0m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mSequential()\n\u001b[0;32m     28\u001b[0m \u001b[39m# Tune the number of units in the first Dense layer\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m# Choose an optimal value between 32-512\u001b[39;00m\n\u001b[0;32m     30\u001b[0m model\u001b[39m.\u001b[39madd(\n\u001b[0;32m     31\u001b[0m     keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\n\u001b[0;32m     32\u001b[0m         units\u001b[39m=\u001b[39mhp\u001b[39m.\u001b[39mInt(\u001b[39m'\u001b[39m\u001b[39munits_layer\u001b[39m\u001b[39m'\u001b[39m, min_value\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, max_value\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m, step\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m),\n\u001b[0;32m     33\u001b[0m         activation\u001b[39m=\u001b[39mhp\u001b[39m.\u001b[39mChoice(\n\u001b[0;32m     34\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mdense_activation\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     35\u001b[0m             values\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtanh\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     36\u001b[0m             default\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m---> 37\u001b[0m         input_shape\u001b[39m=\u001b[39minput_shape\n\u001b[0;32m     38\u001b[0m     )\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m model\u001b[39m.\u001b[39madd(\n\u001b[0;32m     43\u001b[0m     keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDropout(\n\u001b[0;32m     44\u001b[0m         rate\u001b[39m=\u001b[39mhp\u001b[39m.\u001b[39mFloat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     )\n\u001b[0;32m     51\u001b[0m )\n\u001b[0;32m     53\u001b[0m model\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mDense(\u001b[39m3\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_shape' is not defined"
     ]
    }
   ],
   "source": [
    "if model_type == 1:\n",
    "    print(input_shapex)\n",
    "    hypermodel_1 = RegressionHyperModel_1Layer(input_shape=input_shapex)\n",
    "    print(hypermodel_1)\n",
    "    tuner_hb1 = kt.Hyperband(\n",
    "                    hypermodel_1,\n",
    "                    objective=kt.Objective(\"val_recall\", direction=\"max\"),\n",
    "                    max_epochs=1000,\n",
    "                    factor=3,\n",
    "                    directory='my_dir_model1',\n",
    "                    project_name='Estimator_v3')\n",
    "else:\n",
    "    print(\"Option not available\")\n",
    "    exit(-1)\n",
    "                    \n",
    "                    \n",
    "# Will stop training if the \"val_loss\" hasn't improved in 5 epochs.\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_recall', patience=10)\n",
    "\n",
    "\n",
    "if model_type == 1:\n",
    "    tuner_hb1.search(X_train, Y_train, epochs=2000, validation_split=0.2, callbacks=[stop_early], verbose=1)\n",
    "    tuner_hb1.results_summary()\n",
    "    best_hps1=tuner_hb1.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                  mode='min',\n",
    "                                  patience=10,\n",
    "                                  verbose=1,\n",
    "                                  restore_best_weights=True)\n",
    "]\n",
    "if model_type == 1:\n",
    "    model1 = tuner_hb1.hypermodel.build(best_hps1)\n",
    "    history1 = model1.fit(X_train, Y_train, epochs=1000, callbacks=callbacks, validation_split=0.2)\n",
    "    eval_result1 = model1.evaluate(X_test, Y_test)\n",
    "    print(\"[test loss, test recall, test precision]:\", eval_result1)\n",
    "\n",
    "if model_type == 1:\n",
    "    y_pred = model1.predict(X_test)\n",
    "\n",
    "y_pred=np.argmax(y_pred, axis=1)\n",
    "Y_test=np.argmax(Y_test, axis=1)\n",
    "\n",
    "print(\"f1_score: %0.5f\" % (f1_score(Y_test, y_pred, average=\"weighted\")) )\n",
    "print(\"precision_score: %0.5f\" % (precision_score(Y_test, y_pred, average=\"weighted\")) )\n",
    "print(\"recall_score: %0.5f\" % (recall_score(Y_test, y_pred, average=\"weighted\")) )\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "#print(cm)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.array(['Plain','Edge','Texture']))\n",
    "disp.plot()\n",
    "plt.savefig('confusion_matrix_Layer' + str(model_type) + '.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Obtenemos el numero total de espectro en el train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totalSpectrums = FilesNumLines(trainFiles)\n",
    "print(\"Número total de espectros en todos los ficheros: \",totalSpectrums)\n",
    "# totalSpectrums = SpectrumsCounter(trainFiles)\n",
    "# print(totalSpectrums)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### DEFINICION DEL MODELO de RED\n",
    "\n",
    "Nota: LA ultima Capa debe tener el numero de neuronas equivalente al total de materiales a distinguir.\n",
    "Ojo si metemos la Cinta serían 4, Se podría obtener el numero de clases del BatchGenerator? SI, con n_clases obtenido del trainGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "# model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
    "# model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(n_clases,activation='sigmoid'))\n",
    "\n",
    "# summarize the model\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "# simple early stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "es = EarlyStopping(monitor='loss', mode='auto', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='sparse_categorical_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "# fit model\n",
    "history= model.fit(traingenerator,epochs=100, batch_size=500, steps_per_epoch=10,  verbose=1, callbacks=[es, mc])\n",
    "#history = model.fit(x,y, epochs=100, batch_size=500, steps_per_epoch=10,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "saved_model = load_model('best_model.h5')\n",
    "# evaluate the model\n",
    "#_, train_acc = saved_model.evaluate(trainX, trainy, verbose=0)\n",
    "#_, test_acc = saved_model.evaluate(testX, testy, verbose=0)\n",
    "#print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "plt.title('Learning Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "#plt.plot(history.history['val_loss'], label='train')\n",
    "plt.plot(history.history['sparse_categorical_accuracy'], label='acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filelist=testFiles\n",
    "validationGenerator = ValidationGenerator(filelist, batchsize, ';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loss, accuracy= model.evaluate(x,y)\n",
    "loss, accuracy= saved_model.evaluate(validationGenerator)\n",
    "print('Model Loss: %.2f, Accuracy: %.2f' % ((loss*100),(accuracy*100)))\n",
    "#y_pred= model.predict(x,None);\n",
    "\n",
    "#print(y_pred)\n",
    "\n",
    "#loss, acc = model.evaluate(x,y, verbose=1, batch_size=100)\n",
    "#print('Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### INFORME DE RESULTADOS OBTENIDOS - VALIDACION\n",
    "\n",
    "Datos Nuevos capturados con la cinta negra - JOVISA_CSV_FOLDER\n",
    "----------------------------------------\n",
    "1- SIN TENER EN CUENTA LA CINTA NEGRA EN LOS DATOS DE ENTRADA\n",
    "\n",
    "    a) Maxnorm : Loss: 5.18%, Accuracy: 99.83%\n",
    "\n",
    "    b) NO Maxnorm: Loss: 4.45 %, Accuracy: 99.84%\n",
    "\n",
    "    c) raw: Loss: 109.16%, Accuracy: 42.37%\n",
    "\n",
    "2- INCLUYENDO LA CINTA NEGRA EN LOS DATOS DE ENTRADA\n",
    "\n",
    "    a) Maxnorm : Loss: 9.92%, Accuracy: 96.61%\n",
    "\n",
    "    b) NO Maxnorm: Loss: 16.71%, Accuracy: 92.23%\n",
    "\n",
    "    c) raw: Loss: 247.41%, Accuracy: 87.11%\n",
    "\n",
    "Datos caputados con la cinta AZUL -\n",
    "----------------------------------------\n",
    "1- SIN TENER EN CUENTA LA CINTA AZUL EN LOS DATOS DE ENTRADA\n",
    "\n",
    "    a) Maxnorm : Loss: 1.69%, Accuracy: 99.62%\n",
    "\n",
    "    b) NO Maxnorm: Loss:  4.54%, Accuracy: 98.96%\n",
    "\n",
    "    c) raw: Loss: 543.15%, Accuracy: 96.76%\n",
    "2- INCLUYENDO LA CINTA azul EN LOS DATOS DE ENTRADA\n",
    "\n",
    "    a) Maxnorm : Loss: 26.4%, Accuracy: 91.96%\n",
    "\n",
    "    b) NO Maxnorm: Loss:  45.77%, Accuracy:81.02%\n",
    "\n",
    "    c) raw: Loss: 5212.28%, Accuracy: 5.56%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "estimator = KerasClassifier(build_fn=model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import recall_score, cohen_kappa_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "#A=confusion_matrix(y, y_pred)\n",
    "#disp = ConfusionMatrixDisplay(confusion_matrix=A)\n",
    "#disp.plot()\n",
    "\n",
    "\n",
    "#print(accuracy_score(Alltest_labels, Allpredict))\n",
    "#print(recall_score(y, y_pred, average=None))\n",
    "#print(cohen_kappa_score(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Sanity Check\n",
    "\n",
    "Ejecutando la siguiente celda (varias veces) deberemos ver en colores diferentes distintos espectros.\n",
    "Se obtienen en orden desde el BatchGenerator pero se podrían poner aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sanity Check ################################################\n",
    "# Cargamos un único batch.\n",
    "for x, y in generator:\n",
    "    print(x.shape, y.shape)\n",
    "    numLamdas=x.shape[0]\n",
    "    print(numLamdas)\n",
    "    print(x,y)\n",
    "    plt.figure\n",
    "    for ix in range(numLamdas):\n",
    "        #Ploteamos el spectrum si 1 - BOTELLA --> ROJO\n",
    "        if y[ix]==0.:\n",
    "            spectrum=x[ix]\n",
    "            print(\"BOTELLA\")\n",
    "            plt.plot(spectrum, \"r-\")\n",
    "\n",
    "    for ix in range(numLamdas):\n",
    "        #Ploteamos el spectrum si 2 - PAPEL --> AZUL\n",
    "        if y[ix]==1.:\n",
    "            spectrum=x[ix]\n",
    "            print(\"PAPEL\")\n",
    "            plt.plot(spectrum, \"b-\")\n",
    "\n",
    "    for ix in range(numLamdas):\n",
    "        #Ploteamos el spectrum si 3 - CINTA --> NEGRO\n",
    "        if y[ix]==2.:\n",
    "            spectrum=x[ix]\n",
    "            print(\"CINTA\")\n",
    "            plt.plot(spectrum, \"k-\")\n",
    "\n",
    "    for ix in range(numLamdas):\n",
    "        #Ploteamos el spectrum si 2 - TAPON --> CYAN\n",
    "        if y[ix]==3.:\n",
    "            spectrum=x[ix]\n",
    "            print(\"TAPON\")\n",
    "            plt.plot(spectrum, \"c-\")\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Definición del Modelo\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Redefinimos el BatchGenerator para que extraiga de todo el train set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# # compile the model with other optimizer\n",
    "# sgd = SGD(learning_rate=0.01, momentum=0.9)\n",
    "sgd = SGD(learning_rate=0.001)\n",
    "model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
