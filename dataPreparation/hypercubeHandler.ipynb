{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the needed libraries ( hipercube class and hypercubes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypercube import Hypercube\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from squares import SquareDetector\n",
    "from segment_anything_meta import Segmenter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize hypercubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Add your own values { run: \"auto\" }\n",
    "\n",
    "path = \"C:\\\\Users\\\\Fran\\\\Downloads\\\\Hypercubos\\\\Hypercubos\\\\ABS\" # @param {type:\"string\"}\n",
    "hypercube = Hypercube()\n",
    "\n",
    "all_files = glob.glob(os.path.join(path , \"*.bin\"))\n",
    "print(len(all_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "Here we will make use of the preprocess library in order to apply different filters and test the behavior of the net in different circumstances. \n",
    "In order to make use of this library we should know what every filter is used for and apply it proper, for that pourpose I recommend to have a look to the library documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(1, r'C:\\Users\\Fran\\Projects\\Tfg_IA\\Preprocess')\n",
    "import preprocess as preprocess\n",
    "import matplotlib.pyplot as plt\n",
    "ncomponents = 3\n",
    "hipercubo = Hypercube()\n",
    "hipercubo.Load(all_files[0])\n",
    "hipercubo.Crop(94,528,10,214)\n",
    "hipercubo.average()\n",
    "print(hipercubo.hipercubo.shape)\n",
    "pc_img = preprocess.pca(hipercubo,100,ncomponents)\n",
    "print(pc_img.shape)\n",
    "for i in range(ncomponents):\n",
    "    plt.title(f'PC - {i}')\n",
    "    plt.imshow(pc_img[i,:,:], cmap='nipy_spectral')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(1, r'C:\\Users\\Fran\\Projects\\Tfg_IA\\Preprocess')\n",
    "import preprocess as preprocess\n",
    "import matplotlib.pyplot as plt\n",
    "hipercubo = Hypercube()\n",
    "hipercubo.Load(all_files[0])\n",
    "hipercubo.Crop(94,528,10,214)\n",
    "hipercubo.average()\n",
    "hipercubo.PlotIMGBidimensional()\n",
    "print(hipercubo.hipercubo.max(),hipercubo.hipercubo.min())\n",
    "# hipercubo.plotspectres()\n",
    "\n",
    "# Plot SNV\n",
    "standardNormalizated = preprocess.standardNormalizationVariate(hipercubo)\n",
    "for i in range(len(standardNormalizated)):\n",
    "    plt.plot(standardNormalizated[i])\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different point spectres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(1, r'C:\\Users\\Fran\\Projects\\Tfg_IA\\Preprocess')\n",
    "import preprocess as preprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from ponit_selector import PointSelector\n",
    "hipercubo = Hypercube()\n",
    "hipercubo.Load(all_files[0])\n",
    "hipercubo.Crop(94,528,10,214)\n",
    "hipercubo.average()\n",
    "hipercubo.PlotIMGBidimensional()\n",
    "# hipercubo.plotspectres()\n",
    "\n",
    "# Select Roi from hipercubo.hipercubo and plot the selected points\n",
    "image = hipercubo.returnImage()\n",
    "selector = PointSelector(image, \"Selecciona los puntos\")\n",
    "selector.show_and_wait()\n",
    "for i in range(len(selector.points)):\n",
    "    plt.plot(hipercubo.hipercubo[selector.points[i][1],:,selector.points[i][0]])\n",
    "plt.show()\n",
    "selector = PointSelector(image, \"Selecciona los puntos\")\n",
    "selector.show_and_wait()\n",
    "for i in range(len(selector.points)):\n",
    "    plt.plot(hipercubo.hipercubo[selector.points[i][1],:,selector.points[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find max and min values in the spectres to find while filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxValue = 0\n",
    "minValue = 1000\n",
    "for filename in all_files:\n",
    "    hipercubo = Hypercube()\n",
    "    hipercubo.Load(filename)\n",
    "    hipercubo.median()\n",
    "    if maxValue < np.max(hipercubo.medianHipercube):\n",
    "        maxValue = np.max(hipercubo.medianHipercube)\n",
    "    if minValue > np.min(hipercubo.medianHipercube): \n",
    "        minValue = np.min(hipercubo.medianHipercube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First way to preprocess ( Obtaining each pixel classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Add your own values { display-mode: \"form\" }\n",
    "y1 = 94 # @param {type:\"integer\"}\n",
    "y2 = 528 # @param {type:\"integer\"}\n",
    "x1 = 10 # @param {type:\"integer\"}\n",
    "x2 = 214 # @param {type:\"integer\"}\n",
    "annotation = \"pp_pe\" # @param [\"pp_pe\", \"abs\", \"pet\"] {allow-input: true}\n",
    "\n",
    "for filename in all_files:\n",
    "    hipercubo = Hypercube()\n",
    "    hipercubo.Load(filename)\n",
    "    hipercubo.Crop(y1,y2,x1,x2)\n",
    "    # hipercubo.PlotIMG(100)\n",
    "    hipercubo.average()\n",
    "    # hipercubo.PlotDimLine(100)\n",
    "    # hipercubo.PlotIMG(100)\n",
    "    # hipercubo.PlotIMGBidimensional()\n",
    "    # hipercubo.selectROI()\n",
    "    # hipercubo.PlotIMG(100)\n",
    "    segment = Segmenter(\"vit_h\",r\"sam_vit_h_4b8939.pth\")\n",
    "    image = hipercubo.returnImage()\n",
    "    mask = segment.segmentRoi(image)\n",
    "    finalmask,maskLabeled = hipercubo.extractValues(mask)\n",
    "    hipercubo.saveValues(finalmask, annotation)\n",
    "    hipercubo.saveMask(maskLabeled,filename)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second way to process (obtaining the hole ibage labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for filename in all_files:\n",
    "    hipercubo = Hypercube()\n",
    "    hipercubo.Load(filename)\n",
    "    hipercubo.Crop(94,528,10,214)\n",
    "    # hipercubo.PlotIMG(100)\n",
    "    hipercubo.average()\n",
    "    # hipercubo.PlotDimLine(100)\n",
    "    # hipercubo.PlotIMG(100)\n",
    "    # hipercubo.PlotIMGBidimensional()\n",
    "    # hipercubo.selectROI()\n",
    "    # hipercubo.PlotIMG(100)\n",
    "    segment = SquareDetector(200)\n",
    "    image = hipercubo.returnImage()\n",
    "    mask = segment.selectSquares(image)\n",
    "    finalmask,maskLabeled = hipercubo.extractSquares(mask)\n",
    "    for i in range(len(finalmask)):\n",
    "        hipercubo.saveValues(finalmask[i], r\"C:\\Users\\Fran Quiles\\Documents\\Projects\\Tfg_IA\\Classifiers\\CNN\\SPLIT\\pp_pe\"+ str(counter) + str(i))\n",
    "    print(\"Done\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third way to label \n",
    "Here the idea is to make use of external applications that already exist in the market. What we want to do is to take profit of the Saas that we already have in order to annotate images. I have noticed that what we are really anotating is an image that we preprocess to have a better comprenhension about what the data looks like. What we are going to do here is:\n",
    "- Charge the files\n",
    "- preprocess the images\n",
    "    - Spatial crop\n",
    "    - Average\n",
    "    - Some preprocess to improve the image view to the human being\n",
    "- Export image to an image format\n",
    "\n",
    "Once we have the image preprocess to be process by a human what we have to do is to send the image to our labeling machine, we have a bunch of labeling softawares:\n",
    "- AWS\n",
    "- Microsoft azure\n",
    "- Roboflow\n",
    "- CVAT\n",
    "- Label Box\n",
    "- Scale AI\n",
    "- Make Sense\n",
    "- VGG Image Annotator\n",
    "- Labelme\n",
    "- Dashdoodler\n",
    "- Label Studio\n",
    "\n",
    "There are some of them that we can simple run in a docker image locally, so depending on what tool we prefer we should send the images in one or another way.\n",
    "\n",
    "Once we have our images labeleds depending on what is the format labeled we have to create a  converter to match this information with our data, depending on what we really want."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hipercubo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
